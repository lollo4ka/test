{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-e723d3eb-fec1-4ff1-bf8e-6873e38e09e2","deepnote_to_be_reexecuted":false,"source_hash":"f025f0b1","execution_millis":127,"execution_start":1613126028179,"deepnote_cell_type":"code"},"source":"# Start writing code here...\n# naive approach to normalizing the data before splitting the data and evaluating the model\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n    random_state=7)\n# standardize the dataset\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1) # fit the model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n# evaluate the model\nyhat = model.predict(X_test)\n# evaluate predictions\naccuracy = accuracy_score(y_test, yhat)\nprint('Accuracy: %.3f' % (accuracy*100))","outputs":[{"name":"stdout","text":"Accuracy: 84.848\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-e5e99c2c-883b-4f9e-a99a-1858c2156cdf","deepnote_to_be_reexecuted":false,"source_hash":"8db0ab5f","execution_millis":36,"execution_start":1613126151912,"deepnote_cell_type":"code"},"source":"# correct approach for normalizing the data after the data is split before the model is\n# evaluated\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n    random_state=7)\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n# define the scaler\nscaler = MinMaxScaler()\n# fit on the training dataset\nscaler.fit(X_train)\n# scale the training dataset\nX_train = scaler.transform(X_train)\n# scale the test dataset\nX_test = scaler.transform(X_test)\n# fit the model\nmodel = LogisticRegression() \nmodel.fit(X_train, y_train)\n# evaluate the model\nyhat = model.predict(X_test)\n# evaluate predictions\naccuracy = accuracy_score(y_test, yhat) \nprint('Accuracy: %.3f' % (accuracy*100))","outputs":[{"name":"stdout","text":"Accuracy: 85.455\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-e775dc53-a60b-4d0c-97ca-8aafb41f2c3e","deepnote_to_be_reexecuted":false,"source_hash":"83eaaf02","execution_millis":1470,"execution_start":1613132885785,"deepnote_cell_type":"code"},"source":"# correct data preparation for model evaluation with k-fold cross-validation\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n    random_state=7)\n# define the pipeline\nsteps = list()\nsteps.append(('scaler', MinMaxScaler()))\nsteps.append(('model', LogisticRegression()))\npipeline = Pipeline(steps=steps)\n# define the evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate the model using cross-validation\nscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n# report performance\nprint('Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))","outputs":[{"name":"stdout","text":"Accuracy: 85.433 (3.471)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-4ead0f61-0796-48be-8e42-bf9d668dedf5","deepnote_to_be_reexecuted":false,"source_hash":"a45e4c02","execution_millis":282,"execution_start":1613133333221,"deepnote_cell_type":"code"},"source":"# summarize the number of unique values for each column using numpy\nfrom numpy import loadtxt\nfrom numpy import unique\n# load the dataset\ndata = loadtxt('oil-spill.csv', delimiter=',')\n# summarize the number of unique values in each column for i in range(data.shape[1]):\nprint(i, len(unique(data[:, i])))","outputs":[{"output_type":"error","ename":"OSError","evalue":"oil-spill.csv not found.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-a6711356d8a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'oil-spill.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# summarize the number of unique values in each column for i in range(data.shape[1]):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    533\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    534\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: oil-spill.csv not found."]}],"execution_count":null},{"cell_type":"code","source":"# summarize the number of unique values for each column using numpy\nfrom pandas import read_csv\n# load the dataset\ndf = read_csv('oil-spill.csv', header=None)\n# summarize the number of unique values in each column \nprint(df.nunique())","metadata":{"tags":[],"cell_id":"00004-4f032d7b-d3ee-447e-887b-9c1bbc01ae39","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# delete columns with a single unique value\nfrom pandas import read_csv\n# load the dataset\ndf = read_csv('oil-spill.csv', header=None) print(df.shape)\n# get number of unique values for each column counts = df.nunique()\n# record columns to delete\nto_del = [i for i,v in enumerate(counts) if v == 1]\nprint(to_del)\n# drop useless columns\ndf.drop(to_del, axis=1, inplace=True)\nprint(df.shape)","metadata":{"tags":[],"cell_id":"00005-c8b61b32-e7ff-49f5-8316-c8f6fc2ed751","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# summarize the percentage of unique values for each column using numpy\nfrom numpy import loadtxt\nfrom numpy import unique\n# load the dataset\ndata = loadtxt('oil-spill.csv', delimiter=',')\n# summarize the number of unique values in each column for i in range(data.shape[1]):\n  num = len(unique(data[:, i]))\n  percentage = float(num) / data.shape[0] * 100\n  if percentage < 1:\nprint('%d, %d, %.1f%%' % (i, num, percentage))","metadata":{"tags":[],"cell_id":"00006-e232ddba-dcd3-4efc-b57f-ed7641396e7b","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# explore the effect of the variance thresholds on the number of selected features\nfrom numpy import arange\nfrom pandas import read_csv\nfrom sklearn.feature_selection import VarianceThreshold from matplotlib import pyplot\n# load the dataset\ndf = read_csv('oil-spill.csv', header=None)\n# split data into inputs and outputs\ndata = df.values\nX = data[:, :-1]\ny = data[:, -1]\nprint(X.shape, y.shape)\n# define thresholds to check\nthresholds = arange(0.0, 0.55, 0.05)\n# apply transform with each threshold\nresults = list()\nfor t in thresholds:\n  # define the transform\n  transform = VarianceThreshold(threshold=t)\n  # transform the input data\n  X_sel = transform.fit_transform(X)\n  # determine the number of input features\nn_features = X_sel.shape[1]\nprint('>Threshold=%.2f, Features=%d' % (t, n_features)) # store the result\nresults.append(n_features)\n# plot the threshold vs the number of selected features\npyplot.plot(thresholds, results)\npyplot.show()","metadata":{"tags":[],"cell_id":"00007-7a11f333-7528-4c84-b61f-cef56c06a4ea","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# evaluate model on training dataset with outliers removed\nfrom pandas import read_csv\nfrom sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.neighbors import LocalOutlierFactor from sklearn.metrics import mean_absolute_error\n# load the dataset\ndf = read_csv('housing.csv', header=None)\n# retrieve the array\ndata = df.values\n# split into input and output elements\nX, y = data[:, :-1], data[:, -1]\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n# summarize the shape of the training dataset\nprint(X_train.shape, y_train.shape)\n# identify outliers in the training dataset\nlof = LocalOutlierFactor()\nyhat = lof.fit_predict(X_train)\n# select all rows that are not outliers\nmask = yhat != -1\nX_train, y_train = X_train[mask, :], y_train[mask]\n# summarize the shape of the updated training dataset print(X_train.shape, y_train.shape)\n# fit the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n# evaluate the model\nyhat = model.predict(X_test)\n# evaluate predictions\nmae = mean_absolute_error(y_test, yhat)\nprint('MAE: %.3f' % mae)","metadata":{"tags":[],"cell_id":"00008-350eaf96-e3c8-419f-af5b-108f8e8377d6","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# compare statistical imputation strategies for the horse colic dataset\nfrom numpy import mean\nfrom numpy import std\nfrom pandas import read_csv\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom matplotlib import pyplot\n# load dataset\ndataframe = read_csv('horse-colic.csv', header=None, na_values='?') # split into input and output elements\ndata = dataframe.values\nix = [i for i in range(data.shape[1]) if i != 23]\nX, y = data[:, ix], data[:, 23]\n# evaluate each strategy on the dataset\nresults = list()\nstrategies = ['mean', 'median', 'most_frequent', 'constant']\nfor s in strategies:\n  # create the modeling pipeline\npipeline = Pipeline(steps=[('i', SimpleImputer(strategy=s)), ('m', RandomForestClassifier())])\n  # evaluate the model\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) # store results\nresults.append(scores)\nprint('>%s %.3f (%.3f)' % (s, mean(scores), std(scores)))\n# plot model performance for comparison\npyplot.boxplot(results, labels=strategies, showmeans=True)\npyplot.show()","metadata":{"tags":[],"cell_id":"00009-61ac8c2a-0e8f-4f86-b64d-4c904738e23b","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=1f8a4f81-220a-441c-aaed-8cf3b2c42f5b' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"893f7aec-6c1a-4c36-8f63-b456ee1919d7","deepnote":{},"deepnote_execution_queue":[]}}